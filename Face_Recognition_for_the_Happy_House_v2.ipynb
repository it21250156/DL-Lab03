{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNoAY1Hlp+AnzPFQKp8UP8n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/it21250156/DL-Lab03/blob/main/Face_Recognition_for_the_Happy_House_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJhVjiXCd8BU",
        "outputId": "a78855c8-0544-4720-b19d-c38f92ede4d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ],
      "source": [
        "# %% [code]\n",
        "%tensorflow_version 2.x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip weights.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e578wP0-fbhi",
        "outputId": "86e82d46-cfd2-4ed3-b06b-49b298ad5536"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  weights.zip\n",
            "   creating: weights/\n",
            "  inflating: weights/bn1_b.csv       \n",
            "  inflating: weights/bn1_m.csv       \n",
            "  inflating: weights/bn1_v.csv       \n",
            "  inflating: weights/bn1_w.csv       \n",
            "  inflating: weights/bn2_b.csv       \n",
            "  inflating: weights/bn2_m.csv       \n",
            "  inflating: weights/bn2_v.csv       \n",
            "  inflating: weights/bn2_w.csv       \n",
            "  inflating: weights/bn3_b.csv       \n",
            "  inflating: weights/bn3_m.csv       \n",
            "  inflating: weights/bn3_v.csv       \n",
            "  inflating: weights/bn3_w.csv       \n",
            "  inflating: weights/conv1_b.csv     \n",
            "  inflating: weights/conv1_w.csv     \n",
            "  inflating: weights/conv2_b.csv     \n",
            "  inflating: weights/conv2_w.csv     \n",
            "  inflating: weights/conv3_b.csv     \n",
            "  inflating: weights/conv3_w.csv     \n",
            "  inflating: weights/dense_b.csv     \n",
            "  inflating: weights/dense_w.csv     \n",
            "  inflating: weights/inception_3a_1x1_bn_b.csv  \n",
            "  inflating: weights/inception_3a_1x1_bn_m.csv  \n",
            "  inflating: weights/inception_3a_1x1_bn_v.csv  \n",
            "  inflating: weights/inception_3a_1x1_bn_w.csv  \n",
            "  inflating: weights/inception_3a_1x1_conv_b.csv  \n",
            "  inflating: weights/inception_3a_1x1_conv_w.csv  \n",
            "  inflating: weights/inception_3a_3x3_bn1_b.csv  \n",
            "  inflating: weights/inception_3a_3x3_bn1_m.csv  \n",
            "  inflating: weights/inception_3a_3x3_bn1_v.csv  \n",
            "  inflating: weights/inception_3a_3x3_bn1_w.csv  \n",
            "  inflating: weights/inception_3a_3x3_bn2_b.csv  \n",
            "  inflating: weights/inception_3a_3x3_bn2_m.csv  \n",
            "  inflating: weights/inception_3a_3x3_bn2_v.csv  \n",
            "  inflating: weights/inception_3a_3x3_bn2_w.csv  \n",
            "  inflating: weights/inception_3a_3x3_conv1_b.csv  \n",
            "  inflating: weights/inception_3a_3x3_conv1_w.csv  \n",
            "  inflating: weights/inception_3a_3x3_conv2_b.csv  \n",
            "  inflating: weights/inception_3a_3x3_conv2_w.csv  \n",
            "  inflating: weights/inception_3a_5x5_bn1_b.csv  \n",
            "  inflating: weights/inception_3a_5x5_bn1_m.csv  \n",
            "  inflating: weights/inception_3a_5x5_bn1_v.csv  \n",
            "  inflating: weights/inception_3a_5x5_bn1_w.csv  \n",
            "  inflating: weights/inception_3a_5x5_bn2_b.csv  \n",
            "  inflating: weights/inception_3a_5x5_bn2_m.csv  \n",
            "  inflating: weights/inception_3a_5x5_bn2_v.csv  \n",
            "  inflating: weights/inception_3a_5x5_bn2_w.csv  \n",
            "  inflating: weights/inception_3a_5x5_conv1_b.csv  \n",
            "  inflating: weights/inception_3a_5x5_conv1_w.csv  \n",
            "  inflating: weights/inception_3a_5x5_conv2_b.csv  \n",
            "  inflating: weights/inception_3a_5x5_conv2_w.csv  \n",
            "  inflating: weights/inception_3a_pool_bn_b.csv  \n",
            "  inflating: weights/inception_3a_pool_bn_m.csv  \n",
            "  inflating: weights/inception_3a_pool_bn_v.csv  \n",
            "  inflating: weights/inception_3a_pool_bn_w.csv  \n",
            "  inflating: weights/inception_3a_pool_conv_b.csv  \n",
            "  inflating: weights/inception_3a_pool_conv_w.csv  \n",
            "  inflating: weights/inception_3b_1x1_bn_b.csv  \n",
            "  inflating: weights/inception_3b_1x1_bn_m.csv  \n",
            "  inflating: weights/inception_3b_1x1_bn_v.csv  \n",
            "  inflating: weights/inception_3b_1x1_bn_w.csv  \n",
            "  inflating: weights/inception_3b_1x1_conv_b.csv  \n",
            "  inflating: weights/inception_3b_1x1_conv_w.csv  \n",
            "  inflating: weights/inception_3b_3x3_bn1_b.csv  \n",
            "  inflating: weights/inception_3b_3x3_bn1_m.csv  \n",
            "  inflating: weights/inception_3b_3x3_bn1_v.csv  \n",
            "  inflating: weights/inception_3b_3x3_bn1_w.csv  \n",
            "  inflating: weights/inception_3b_3x3_bn2_b.csv  \n",
            "  inflating: weights/inception_3b_3x3_bn2_m.csv  \n",
            "  inflating: weights/inception_3b_3x3_bn2_v.csv  \n",
            "  inflating: weights/inception_3b_3x3_bn2_w.csv  \n",
            "  inflating: weights/inception_3b_3x3_conv1_b.csv  \n",
            "  inflating: weights/inception_3b_3x3_conv1_w.csv  \n",
            "  inflating: weights/inception_3b_3x3_conv2_b.csv  \n",
            "  inflating: weights/inception_3b_3x3_conv2_w.csv  \n",
            "  inflating: weights/inception_3b_5x5_bn1_b.csv  \n",
            "  inflating: weights/inception_3b_5x5_bn1_m.csv  \n",
            "  inflating: weights/inception_3b_5x5_bn1_v.csv  \n",
            "  inflating: weights/inception_3b_5x5_bn1_w.csv  \n",
            "  inflating: weights/inception_3b_5x5_bn2_b.csv  \n",
            "  inflating: weights/inception_3b_5x5_bn2_m.csv  \n",
            "  inflating: weights/inception_3b_5x5_bn2_v.csv  \n",
            "  inflating: weights/inception_3b_5x5_bn2_w.csv  \n",
            "  inflating: weights/inception_3b_5x5_conv1_b.csv  \n",
            "  inflating: weights/inception_3b_5x5_conv1_w.csv  \n",
            "  inflating: weights/inception_3b_5x5_conv2_b.csv  \n",
            "  inflating: weights/inception_3b_5x5_conv2_w.csv  \n",
            "  inflating: weights/inception_3b_pool_bn_b.csv  \n",
            "  inflating: weights/inception_3b_pool_bn_m.csv  \n",
            "  inflating: weights/inception_3b_pool_bn_v.csv  \n",
            "  inflating: weights/inception_3b_pool_bn_w.csv  \n",
            "  inflating: weights/inception_3b_pool_conv_b.csv  \n",
            "  inflating: weights/inception_3b_pool_conv_w.csv  \n",
            "  inflating: weights/inception_3c_3x3_bn1_b.csv  \n",
            "  inflating: weights/inception_3c_3x3_bn1_m.csv  \n",
            "  inflating: weights/inception_3c_3x3_bn1_v.csv  \n",
            "  inflating: weights/inception_3c_3x3_bn1_w.csv  \n",
            "  inflating: weights/inception_3c_3x3_bn2_b.csv  \n",
            "  inflating: weights/inception_3c_3x3_bn2_m.csv  \n",
            "  inflating: weights/inception_3c_3x3_bn2_v.csv  \n",
            "  inflating: weights/inception_3c_3x3_bn2_w.csv  \n",
            "  inflating: weights/inception_3c_3x3_conv1_b.csv  \n",
            "  inflating: weights/inception_3c_3x3_conv1_w.csv  \n",
            "  inflating: weights/inception_3c_3x3_conv2_b.csv  \n",
            "  inflating: weights/inception_3c_3x3_conv2_w.csv  \n",
            "  inflating: weights/inception_3c_5x5_bn1_b.csv  \n",
            "  inflating: weights/inception_3c_5x5_bn1_m.csv  \n",
            "  inflating: weights/inception_3c_5x5_bn1_v.csv  \n",
            "  inflating: weights/inception_3c_5x5_bn1_w.csv  \n",
            "  inflating: weights/inception_3c_5x5_bn2_b.csv  \n",
            "  inflating: weights/inception_3c_5x5_bn2_m.csv  \n",
            "  inflating: weights/inception_3c_5x5_bn2_v.csv  \n",
            "  inflating: weights/inception_3c_5x5_bn2_w.csv  \n",
            "  inflating: weights/inception_3c_5x5_conv1_b.csv  \n",
            "  inflating: weights/inception_3c_5x5_conv1_w.csv  \n",
            "  inflating: weights/inception_3c_5x5_conv2_b.csv  \n",
            "  inflating: weights/inception_3c_5x5_conv2_w.csv  \n",
            "  inflating: weights/inception_4a_1x1_bn_b.csv  \n",
            "  inflating: weights/inception_4a_1x1_bn_m.csv  \n",
            "  inflating: weights/inception_4a_1x1_bn_v.csv  \n",
            "  inflating: weights/inception_4a_1x1_bn_w.csv  \n",
            "  inflating: weights/inception_4a_1x1_conv_b.csv  \n",
            "  inflating: weights/inception_4a_1x1_conv_w.csv  \n",
            "  inflating: weights/inception_4a_3x3_bn1_b.csv  \n",
            "  inflating: weights/inception_4a_3x3_bn1_m.csv  \n",
            "  inflating: weights/inception_4a_3x3_bn1_v.csv  \n",
            "  inflating: weights/inception_4a_3x3_bn1_w.csv  \n",
            "  inflating: weights/inception_4a_3x3_bn2_b.csv  \n",
            "  inflating: weights/inception_4a_3x3_bn2_m.csv  \n",
            "  inflating: weights/inception_4a_3x3_bn2_v.csv  \n",
            "  inflating: weights/inception_4a_3x3_bn2_w.csv  \n",
            "  inflating: weights/inception_4a_3x3_conv1_b.csv  \n",
            "  inflating: weights/inception_4a_3x3_conv1_w.csv  \n",
            "  inflating: weights/inception_4a_3x3_conv2_b.csv  \n",
            "  inflating: weights/inception_4a_3x3_conv2_w.csv  \n",
            "  inflating: weights/inception_4a_5x5_bn1_b.csv  \n",
            "  inflating: weights/inception_4a_5x5_bn1_m.csv  \n",
            "  inflating: weights/inception_4a_5x5_bn1_v.csv  \n",
            "  inflating: weights/inception_4a_5x5_bn1_w.csv  \n",
            "  inflating: weights/inception_4a_5x5_bn2_b.csv  \n",
            "  inflating: weights/inception_4a_5x5_bn2_m.csv  \n",
            "  inflating: weights/inception_4a_5x5_bn2_v.csv  \n",
            "  inflating: weights/inception_4a_5x5_bn2_w.csv  \n",
            "  inflating: weights/inception_4a_5x5_conv1_b.csv  \n",
            "  inflating: weights/inception_4a_5x5_conv1_w.csv  \n",
            "  inflating: weights/inception_4a_5x5_conv2_b.csv  \n",
            "  inflating: weights/inception_4a_5x5_conv2_w.csv  \n",
            "  inflating: weights/inception_4a_pool_bn_b.csv  \n",
            "  inflating: weights/inception_4a_pool_bn_m.csv  \n",
            "  inflating: weights/inception_4a_pool_bn_v.csv  \n",
            "  inflating: weights/inception_4a_pool_bn_w.csv  \n",
            "  inflating: weights/inception_4a_pool_conv_b.csv  \n",
            "  inflating: weights/inception_4a_pool_conv_w.csv  \n",
            "  inflating: weights/inception_4e_3x3_bn1_b.csv  \n",
            "  inflating: weights/inception_4e_3x3_bn1_m.csv  \n",
            "  inflating: weights/inception_4e_3x3_bn1_v.csv  \n",
            "  inflating: weights/inception_4e_3x3_bn1_w.csv  \n",
            "  inflating: weights/inception_4e_3x3_bn2_b.csv  \n",
            "  inflating: weights/inception_4e_3x3_bn2_m.csv  \n",
            "  inflating: weights/inception_4e_3x3_bn2_v.csv  \n",
            "  inflating: weights/inception_4e_3x3_bn2_w.csv  \n",
            "  inflating: weights/inception_4e_3x3_conv1_b.csv  \n",
            "  inflating: weights/inception_4e_3x3_conv1_w.csv  \n",
            "  inflating: weights/inception_4e_3x3_conv2_b.csv  \n",
            "  inflating: weights/inception_4e_3x3_conv2_w.csv  \n",
            "  inflating: weights/inception_4e_5x5_bn1_b.csv  \n",
            "  inflating: weights/inception_4e_5x5_bn1_m.csv  \n",
            "  inflating: weights/inception_4e_5x5_bn1_v.csv  \n",
            "  inflating: weights/inception_4e_5x5_bn1_w.csv  \n",
            "  inflating: weights/inception_4e_5x5_bn2_b.csv  \n",
            "  inflating: weights/inception_4e_5x5_bn2_m.csv  \n",
            "  inflating: weights/inception_4e_5x5_bn2_v.csv  \n",
            "  inflating: weights/inception_4e_5x5_bn2_w.csv  \n",
            "  inflating: weights/inception_4e_5x5_conv1_b.csv  \n",
            "  inflating: weights/inception_4e_5x5_conv1_w.csv  \n",
            "  inflating: weights/inception_4e_5x5_conv2_b.csv  \n",
            "  inflating: weights/inception_4e_5x5_conv2_w.csv  \n",
            "  inflating: weights/inception_5a_1x1_bn_b.csv  \n",
            "  inflating: weights/inception_5a_1x1_bn_m.csv  \n",
            "  inflating: weights/inception_5a_1x1_bn_v.csv  \n",
            "  inflating: weights/inception_5a_1x1_bn_w.csv  \n",
            "  inflating: weights/inception_5a_1x1_conv_b.csv  \n",
            "  inflating: weights/inception_5a_1x1_conv_w.csv  \n",
            "  inflating: weights/inception_5a_3x3_bn1_b.csv  \n",
            "  inflating: weights/inception_5a_3x3_bn1_m.csv  \n",
            "  inflating: weights/inception_5a_3x3_bn1_v.csv  \n",
            "  inflating: weights/inception_5a_3x3_bn1_w.csv  \n",
            "  inflating: weights/inception_5a_3x3_bn2_b.csv  \n",
            "  inflating: weights/inception_5a_3x3_bn2_m.csv  \n",
            "  inflating: weights/inception_5a_3x3_bn2_v.csv  \n",
            "  inflating: weights/inception_5a_3x3_bn2_w.csv  \n",
            "  inflating: weights/inception_5a_3x3_conv1_b.csv  \n",
            "  inflating: weights/inception_5a_3x3_conv1_w.csv  \n",
            "  inflating: weights/inception_5a_3x3_conv2_b.csv  \n",
            "  inflating: weights/inception_5a_3x3_conv2_m.csv  \n",
            "  inflating: weights/inception_5a_3x3_conv2_v.csv  \n",
            "  inflating: weights/inception_5a_3x3_conv2_w.csv  \n",
            "  inflating: weights/inception_5a_pool_bn_b.csv  \n",
            "  inflating: weights/inception_5a_pool_bn_m.csv  \n",
            "  inflating: weights/inception_5a_pool_bn_v.csv  \n",
            "  inflating: weights/inception_5a_pool_bn_w.csv  \n",
            "  inflating: weights/inception_5a_pool_conv_b.csv  \n",
            "  inflating: weights/inception_5a_pool_conv_w.csv  \n",
            "  inflating: weights/inception_5b_1x1_bn_b.csv  \n",
            "  inflating: weights/inception_5b_1x1_bn_m.csv  \n",
            "  inflating: weights/inception_5b_1x1_bn_v.csv  \n",
            "  inflating: weights/inception_5b_1x1_bn_w.csv  \n",
            "  inflating: weights/inception_5b_1x1_conv_b.csv  \n",
            "  inflating: weights/inception_5b_1x1_conv_w.csv  \n",
            "  inflating: weights/inception_5b_3x3_bn1_b.csv  \n",
            "  inflating: weights/inception_5b_3x3_bn1_m.csv  \n",
            "  inflating: weights/inception_5b_3x3_bn1_v.csv  \n",
            "  inflating: weights/inception_5b_3x3_bn1_w.csv  \n",
            "  inflating: weights/inception_5b_3x3_bn2_b.csv  \n",
            "  inflating: weights/inception_5b_3x3_bn2_m.csv  \n",
            "  inflating: weights/inception_5b_3x3_bn2_v.csv  \n",
            "  inflating: weights/inception_5b_3x3_bn2_w.csv  \n",
            "  inflating: weights/inception_5b_3x3_conv1_b.csv  \n",
            "  inflating: weights/inception_5b_3x3_conv1_w.csv  \n",
            "  inflating: weights/inception_5b_3x3_conv2_b.csv  \n",
            "  inflating: weights/inception_5b_3x3_conv2_w.csv  \n",
            "  inflating: weights/inception_5b_pool_bn_b.csv  \n",
            "  inflating: weights/inception_5b_pool_bn_m.csv  \n",
            "  inflating: weights/inception_5b_pool_bn_v.csv  \n",
            "  inflating: weights/inception_5b_pool_bn_w.csv  \n",
            "  inflating: weights/inception_5b_pool_conv_b.csv  \n",
            "  inflating: weights/inception_5b_pool_conv_w.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [code]\n",
        "import fr_utils"
      ],
      "metadata": {
        "id": "xDaRidkwfizw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [code]\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
        "from keras.models import Model\n",
        "# from keras.layers.normalization import BatchNormalization\n",
        "from tensorflow.keras.layers import BatchNormalization,MaxPooling2D, AveragePooling2D\n",
        "# from keras.layers.merge import Concatenate\n",
        "from tensorflow.keras.layers import concatenate\n",
        "from keras.layers import Lambda, Flatten, Dense\n",
        "from keras.initializers import glorot_uniform\n",
        "# from keras.engine.topology import Layer\n",
        "from tensorflow.keras.layers import Layer, InputSpec\n",
        "from keras import backend as K\n",
        "K.set_image_data_format('channels_first')\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from numpy import genfromtxt\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from fr_utils import *\n",
        "from inception_blocks_v2 import *\n",
        "\n",
        "%matplotlib inline\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4-b9p-wfpbk",
        "outputId": "69dae12c-b290-45cf-a4fe-1be7b34a19e4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.17.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FRmodel = faceRecoModel(input_shape=(3, 96, 96), output_shape=(3,128))\n"
      ],
      "metadata": {
        "id": "EM7NedXEfvbg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [code]\n",
        "print(\"Total Params:\", FRmodel.count_params())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0N-hQwwff0mU",
        "outputId": "9269df1b-58d0-406a-8db1-c06971df3c9e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Params: 3743280\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [code]\n",
        "# GRADED FUNCTION: triplet_loss\n",
        "\n",
        "def triplet_loss(y_true, y_pred, alpha = 0.2):\n",
        "    \"\"\"\n",
        "    Implementation of the triplet loss as defined by formula (3)\n",
        "\n",
        "    Arguments:\n",
        "    y_true -- true labels, required when you define a loss in Keras, you don't need it in this function.\n",
        "    y_pred -- python list containing three objects:\n",
        "            anchor -- the encodings for the anchor images, of shape (None, 128)\n",
        "            positive -- the encodings for the positive images, of shape (None, 128)\n",
        "            negative -- the encodings for the negative images, of shape (None, 128)\n",
        "\n",
        "    Returns:\n",
        "    loss -- real number, value of the loss\n",
        "    \"\"\"\n",
        "\n",
        "    anchor, positive, negative = y_pred[0], y_pred[1], y_pred[2]\n",
        "\n",
        "    ### START CODE HERE ### (≈ 4 lines)\n",
        "    # Step 1: Compute the (encoding) distance between the anchor and the positive\n",
        "    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, positive)))\n",
        "    # Step 2: Compute the (encoding) distance between the anchor and the negative\n",
        "    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, negative)))\n",
        "    # Step 3: subtract the two previous distances and add alpha.\n",
        "    basic_loss = tf.add(tf.subtract(pos_dist, neg_dist), alpha)\n",
        "    # Step 4: Take the maximum of basic_loss and 0.0. Sum over the training examples.\n",
        "    loss = tf.maximum(tf.reduce_mean(basic_loss), 0.0)\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return loss\n"
      ],
      "metadata": {
        "id": "asALYsiyf5me"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [code]\n",
        "# with tf.Session() as test:\n",
        "with tf.compat.v1.Session() as test:\n",
        "#    tf.set_random_seed(1)\n",
        "    tf.random.set_seed(1)\n",
        "    y_true = (None, None, None)\n",
        "    y_pred = (tf.random.normal([3, 128], mean=6, stddev=0.1, seed = 1),\n",
        "              tf.random.normal([3, 128], mean=1, stddev=1, seed = 1),\n",
        "              tf.random.normal([3, 128], mean=3, stddev=4, seed = 1))\n",
        "    loss = triplet_loss(y_true, y_pred)\n",
        "\n",
        "    print(\"loss = \" + str(loss.eval()))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9SaQZfpgMRz",
        "outputId": "bec4b24b-853e-40e0-cc11-22a5dea977ed"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss = 350.02734\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [code]\n",
        "FRmodel.compile(optimizer = 'adam', loss = triplet_loss, metrics = ['accuracy'])\n",
        "load_weights_from_FaceNet(FRmodel)\n"
      ],
      "metadata": {
        "id": "idufR6pLgRq8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [code]\n",
        "database = {}\n",
        "database[\"danielle\"] = img_to_encoding(\"images/danielle.png\", FRmodel)\n",
        "database[\"younes\"] = img_to_encoding(\"images/younes.jpg\", FRmodel)\n",
        "database[\"tian\"] = img_to_encoding(\"images/tian.jpg\", FRmodel)\n",
        "database[\"andrew\"] = img_to_encoding(\"images/andrew.jpg\", FRmodel)\n",
        "database[\"kian\"] = img_to_encoding(\"images/kian.jpg\", FRmodel)\n",
        "database[\"dan\"] = img_to_encoding(\"images/dan.jpg\", FRmodel)\n",
        "database[\"sebastiano\"] = img_to_encoding(\"images/sebastiano.jpg\", FRmodel)\n",
        "database[\"bertrand\"] = img_to_encoding(\"images/bertrand.jpg\", FRmodel)\n",
        "database[\"kevin\"] = img_to_encoding(\"images/kevin.jpg\", FRmodel)\n",
        "database[\"felix\"] = img_to_encoding(\"images/felix.jpg\", FRmodel)\n",
        "database[\"benoit\"] = img_to_encoding(\"images/benoit.jpg\", FRmodel)\n",
        "database[\"arnaud\"] = img_to_encoding(\"images/arnaud.jpg\", FRmodel)\n"
      ],
      "metadata": {
        "id": "ys5yvUswgW8R"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [code]\n",
        "# GRADED FUNCTION: verify\n",
        "\n",
        "def verify(image_path, identity, database, model):\n",
        "    \"\"\"\n",
        "    Function that verifies if the person on the \"image_path\" image is \"identity\".\n",
        "\n",
        "    Arguments:\n",
        "    image_path -- path to an image\n",
        "    identity -- string, name of the person you'd like to verify the identity. Has to be a resident of the Happy house.\n",
        "    database -- python dictionary mapping names of allowed people's names (strings) to their encodings (vectors).\n",
        "    model -- your Inception model instance in Keras\n",
        "\n",
        "    Returns:\n",
        "    dist -- distance between the image_path and the image of \"identity\" in the database.\n",
        "    door_open -- True, if the door should open. False otherwise.\n",
        "    \"\"\"\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "    # Step 1: Compute the encoding for the image. Use img_to_encoding() see example above. (≈ 1 line)\n",
        "    encoding = img_to_encoding(image_path, model)\n",
        "\n",
        "    # Step 2: Compute distance with identity's image (≈ 1 line)\n",
        "    dist = np.linalg.norm(encoding-database[identity])\n",
        "\n",
        "    # Step 3: Open the door if dist < 0.7, else don't open (≈ 3 lines)\n",
        "    if dist < 0.7:\n",
        "        print(\"It's \" + str(identity) + \", welcome home!\")\n",
        "        door_open = True\n",
        "    else:\n",
        "        print(\"It's not \" + str(identity) + \", please go away\")\n",
        "        door_open = False\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return dist, door_open"
      ],
      "metadata": {
        "id": "UFipgJT0g29j"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [code]\n",
        "verify(\"images/camera_0.jpg\", \"younes\", database, FRmodel)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSLWnsoYlRVq",
        "outputId": "8e3115a3-920e-43d1-ae9d-80df21bffe5f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It's younes, welcome home!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6671399, True)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [code]\n",
        "verify(\"images/camera_2.jpg\", \"kian\", database, FRmodel)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tLditsglhDJ",
        "outputId": "3950123d-dd1d-4e33-c7a2-2f49b47a31a8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It's not kian, please go away\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.85868883, False)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [code]\n",
        "# GRADED FUNCTION: who_is_it\n",
        "\n",
        "def who_is_it(image_path, database, model):\n",
        "    \"\"\"\n",
        "    Implements face recognition for the happy house by finding who is the person on the image_path image.\n",
        "\n",
        "    Arguments:\n",
        "    image_path -- path to an image\n",
        "    database -- database containing image encodings along with the name of the person on the image\n",
        "    model -- your Inception model instance in Keras\n",
        "\n",
        "    Returns:\n",
        "    min_dist -- the minimum distance between image_path encoding and the encodings from the database\n",
        "    identity -- string, the name prediction for the person on image_path\n",
        "    \"\"\"\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "    ## Step 1: Compute the target \"encoding\" for the image. Use img_to_encoding() see example above. ## (≈ 1 line)\n",
        "    encoding = img_to_encoding(image_path, model)\n",
        "\n",
        "    ## Step 2: Find the closest encoding ##\n",
        "\n",
        "    # Initialize \"min_dist\" to a large value, say 100 (≈1 line)\n",
        "    min_dist = 100\n",
        "\n",
        "    #save L2 distances to interpret results\n",
        "    output_data = {}\n",
        "\n",
        "    # Loop over the database dictionary's names and encodings.\n",
        "    for (name, db_enc) in database.items():\n",
        "\n",
        "        # Compute L2 distance between the target \"encoding\" and the current \"emb\" from the database. (≈ 1 line)\n",
        "        dist = np.linalg.norm(encoding-db_enc)\n",
        "\n",
        "        #save L2 distances to interpret results\n",
        "        output_data[f'{name}'] = dist\n",
        "\n",
        "        # If this distance is less than the min_dist, then set min_dist to dist, and identity to name. (≈ 3 lines)\n",
        "        if dist < min_dist:\n",
        "            min_dist = dist\n",
        "            identity = name\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    if min_dist > 0.7:\n",
        "        print(\"Not in the database.\")\n",
        "    else:\n",
        "        print (\"it's \" + str(identity) + \", the distance is \" + str(min_dist))\n",
        "\n",
        "    return min_dist, identity, output_data"
      ],
      "metadata": {
        "id": "AjpMFijOlnEQ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [code]\n",
        "output = who_is_it(\"images/camera_0.jpg\", database, FRmodel)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gf9sojhwl6Kq",
        "outputId": "83fce6d0-e1d7-4d60-f603-6ed79cd8a712"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "it's younes, the distance is 0.6671399\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [code]\n",
        "#dictionary contains the L2 distance between target image encoding and database embeddings of other images\n",
        "output[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgJORrWZmK2f",
        "outputId": "780579b6-dfe3-4bd7-f7a9-02ab9cdee212"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'danielle': 1.2107376,\n",
              " 'younes': 0.6671399,\n",
              " 'tian': 1.221301,\n",
              " 'andrew': 0.99589044,\n",
              " 'kian': 0.8566896,\n",
              " 'dan': 0.7684447,\n",
              " 'sebastiano': 0.854448,\n",
              " 'bertrand': 0.7690577,\n",
              " 'kevin': 0.8715994,\n",
              " 'felix': 1.2063178,\n",
              " 'benoit': 0.8959498,\n",
              " 'arnaud': 0.8346038}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [code]\n",
        "from PIL import Image\n",
        "img = Image.open(\"images/ronaldo1.jpg\")\n",
        "img = img.resize((96,96), Image.ANTIALIAS)\n",
        "img.save(\"images/ronaldo1.jpg\")\n",
        "\n",
        "img = Image.open(\"images/ronaldo2.jpg\")\n",
        "img = img.resize((96,96), Image.ANTIALIAS)\n",
        "img.save(\"images/ronaldo2.jpg\")\n",
        "\n",
        "database[\"ronaldo\"] = img_to_encoding(\"images/ronaldo1.jpg\", FRmodel)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7dE3gpHmUfo",
        "outputId": "9d4ec0be-ed96-41f2-c9f2-974c13d46004"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-b183c19872e6>:4: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
            "  img = img.resize((96,96), Image.ANTIALIAS)\n",
            "<ipython-input-22-b183c19872e6>:8: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
            "  img = img.resize((96,96), Image.ANTIALIAS)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [code]\n",
        "verify(\"images/camera_0.jpg\", \"ronaldo\", database, FRmodel)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ca3ufJJrmhB4",
        "outputId": "505a636c-b11c-4252-c39d-62825cd35e10"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It's not ronaldo, please go away\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9850233, False)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [code]\n",
        "output = who_is_it(\"images/ronaldo2.jpg\", database, FRmodel)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfEJc1M6p4F9",
        "outputId": "68c39aa5-db31-469b-f043-e7972c0e9f15"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "it's danielle, the distance is 0.5357453\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [code]\n",
        "output[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PgK15C9qHX0",
        "outputId": "48f64b25-92aa-4f1d-e790-f5532ea0cca0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'danielle': 0.5357453,\n",
              " 'younes': 1.302147,\n",
              " 'tian': 0.54648215,\n",
              " 'andrew': 0.9137047,\n",
              " 'kian': 1.2934924,\n",
              " 'dan': 1.2845762,\n",
              " 'sebastiano': 1.1136677,\n",
              " 'bertrand': 1.3282127,\n",
              " 'kevin': 1.3130096,\n",
              " 'felix': 1.2125703,\n",
              " 'benoit': 0.8989191,\n",
              " 'arnaud': 1.3376323,\n",
              " 'ronaldo': 1.2037222}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aKG-IYxnqMoW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}